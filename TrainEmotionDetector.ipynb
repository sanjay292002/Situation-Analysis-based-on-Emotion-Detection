{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        \"C:/Users/yuvan/OneDrive/Desktop/PROJECT/data/train\",\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "        \"C:/Users/yuvan/OneDrive/Desktop/PROJECT/data/test\",\n",
    "        target_size=(48, 48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 340s 756ms/step - loss: 1.8089 - accuracy: 0.2548 - val_loss: 1.7345 - val_accuracy: 0.3297\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 359s 802ms/step - loss: 1.6410 - accuracy: 0.3586 - val_loss: 1.5495 - val_accuracy: 0.4118\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 349s 778ms/step - loss: 1.5393 - accuracy: 0.4084 - val_loss: 1.4768 - val_accuracy: 0.4410\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 337s 752ms/step - loss: 1.4732 - accuracy: 0.4373 - val_loss: 1.4125 - val_accuracy: 0.4628\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 337s 753ms/step - loss: 1.4115 - accuracy: 0.4616 - val_loss: 1.3639 - val_accuracy: 0.4813\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 337s 751ms/step - loss: 1.3588 - accuracy: 0.4806 - val_loss: 1.3169 - val_accuracy: 0.4987\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 335s 747ms/step - loss: 1.3150 - accuracy: 0.5021 - val_loss: 1.2784 - val_accuracy: 0.5138\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 334s 747ms/step - loss: 1.2713 - accuracy: 0.5226 - val_loss: 1.2481 - val_accuracy: 0.5233\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 336s 749ms/step - loss: 1.2399 - accuracy: 0.5287 - val_loss: 1.2296 - val_accuracy: 0.5324\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 338s 755ms/step - loss: 1.2024 - accuracy: 0.5463 - val_loss: 1.2003 - val_accuracy: 0.5439\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 346s 773ms/step - loss: 1.1734 - accuracy: 0.5580 - val_loss: 1.1810 - val_accuracy: 0.5499\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 339s 756ms/step - loss: 1.1525 - accuracy: 0.5663 - val_loss: 1.1720 - val_accuracy: 0.5555\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 347s 774ms/step - loss: 1.1234 - accuracy: 0.5798 - val_loss: 1.1585 - val_accuracy: 0.5615\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 342s 763ms/step - loss: 1.1021 - accuracy: 0.5877 - val_loss: 1.1319 - val_accuracy: 0.5728\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 343s 765ms/step - loss: 1.0767 - accuracy: 0.5995 - val_loss: 1.1306 - val_accuracy: 0.5716\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 347s 773ms/step - loss: 1.0537 - accuracy: 0.6067 - val_loss: 1.1203 - val_accuracy: 0.5769\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 347s 774ms/step - loss: 1.0309 - accuracy: 0.6145 - val_loss: 1.1005 - val_accuracy: 0.5851\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 342s 763ms/step - loss: 1.0073 - accuracy: 0.6261 - val_loss: 1.0935 - val_accuracy: 0.5910\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 344s 768ms/step - loss: 0.9839 - accuracy: 0.6330 - val_loss: 1.0876 - val_accuracy: 0.5981\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 344s 767ms/step - loss: 0.9612 - accuracy: 0.6413 - val_loss: 1.0879 - val_accuracy: 0.5977\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 377s 841ms/step - loss: 0.9362 - accuracy: 0.6527 - val_loss: 1.0791 - val_accuracy: 0.5985\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 375s 837ms/step - loss: 0.9123 - accuracy: 0.6624 - val_loss: 1.0777 - val_accuracy: 0.5993\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 338s 754ms/step - loss: 0.8985 - accuracy: 0.6670 - val_loss: 1.0710 - val_accuracy: 0.6038\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 340s 758ms/step - loss: 0.8774 - accuracy: 0.6794 - val_loss: 1.0669 - val_accuracy: 0.6041\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 340s 759ms/step - loss: 0.8492 - accuracy: 0.6886 - val_loss: 1.0705 - val_accuracy: 0.6062\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 333s 742ms/step - loss: 0.8326 - accuracy: 0.6924 - val_loss: 1.0615 - val_accuracy: 0.6099\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 332s 741ms/step - loss: 0.8120 - accuracy: 0.7013 - val_loss: 1.0637 - val_accuracy: 0.6137\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 332s 741ms/step - loss: 0.7845 - accuracy: 0.7151 - val_loss: 1.0579 - val_accuracy: 0.6119\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 332s 741ms/step - loss: 0.7634 - accuracy: 0.7213 - val_loss: 1.0761 - val_accuracy: 0.6080\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 331s 739ms/step - loss: 0.7404 - accuracy: 0.7262 - val_loss: 1.0754 - val_accuracy: 0.6108\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 332s 740ms/step - loss: 0.7226 - accuracy: 0.7372 - val_loss: 1.0591 - val_accuracy: 0.6207\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 342s 764ms/step - loss: 0.6984 - accuracy: 0.7464 - val_loss: 1.0913 - val_accuracy: 0.6126\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 329s 735ms/step - loss: 0.6764 - accuracy: 0.7559 - val_loss: 1.0758 - val_accuracy: 0.6208\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 335s 749ms/step - loss: 0.6562 - accuracy: 0.7608 - val_loss: 1.0734 - val_accuracy: 0.6214\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 330s 735ms/step - loss: 0.6460 - accuracy: 0.7648 - val_loss: 1.0753 - val_accuracy: 0.6210\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 330s 736ms/step - loss: 0.6159 - accuracy: 0.7770 - val_loss: 1.0809 - val_accuracy: 0.6229\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 330s 736ms/step - loss: 0.5991 - accuracy: 0.7821 - val_loss: 1.0937 - val_accuracy: 0.6204\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 329s 733ms/step - loss: 0.5781 - accuracy: 0.7934 - val_loss: 1.1056 - val_accuracy: 0.6215\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 329s 734ms/step - loss: 0.5572 - accuracy: 0.7988 - val_loss: 1.1071 - val_accuracy: 0.6249\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 341s 760ms/step - loss: 0.5495 - accuracy: 0.8034 - val_loss: 1.1044 - val_accuracy: 0.6225\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 331s 738ms/step - loss: 0.5185 - accuracy: 0.8107 - val_loss: 1.1220 - val_accuracy: 0.6200\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 332s 741ms/step - loss: 0.5072 - accuracy: 0.8170 - val_loss: 1.1297 - val_accuracy: 0.6207\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 343s 766ms/step - loss: 0.4919 - accuracy: 0.8241 - val_loss: 1.1249 - val_accuracy: 0.6226\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 338s 753ms/step - loss: 0.4735 - accuracy: 0.8293 - val_loss: 1.1464 - val_accuracy: 0.6212\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 350s 781ms/step - loss: 0.4691 - accuracy: 0.8307 - val_loss: 1.1589 - val_accuracy: 0.6236\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 344s 767ms/step - loss: 0.4539 - accuracy: 0.8369 - val_loss: 1.1634 - val_accuracy: 0.6197\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 341s 760ms/step - loss: 0.4298 - accuracy: 0.8452 - val_loss: 1.1651 - val_accuracy: 0.6270\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 339s 756ms/step - loss: 0.4160 - accuracy: 0.8502 - val_loss: 1.1694 - val_accuracy: 0.6223\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 338s 754ms/step - loss: 0.4086 - accuracy: 0.8527 - val_loss: 1.1907 - val_accuracy: 0.6214\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 339s 757ms/step - loss: 0.3929 - accuracy: 0.8558 - val_loss: 1.2001 - val_accuracy: 0.6223\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = emotion_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709//64,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178//64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('emotion_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05cfea83ba036a6eb2c150c0c2cbc27474de2e3ad785bd6b476f65181d24ae79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
